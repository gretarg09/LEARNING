{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [The course](https://www.coursera.org/learn/introduction-to-pymc3/home)\n",
    "* [The course material](https://sjster.github.io/introduction_to_computational_statistics/docs/Production/PyMC3.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. MCMC Metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Paper discussing Bayesian visualization\n",
    "\n",
    "https://arxiv.org/pdf/1709.01449.pdf\n",
    "\n",
    "**[GAG notes]**\n",
    "It's an interesting read.\n",
    "\n",
    "The idea behind posterior predictive checkint is simple: if a model is a good fit we should be able to sue it to generate dasta that resemble the data we observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1.2 Tuning\n",
    "\n",
    "[Colin Caroll's Talk](https://sjster.github.io/introduction_to_computational_statistics/docs/Production/PyMC3.html)\n",
    "\n",
    "When a step size is required, Pymc3 uses the first 500 steps varying the step size to get to an acceptance rate of $23.4%$.\n",
    "\n",
    "* The acceptance rate is the proportion of proposed values that are not rejected during the sampling process. Refer to Course 2, Metropolis Algorithms for more information.\n",
    "* In Metropolis algorithms, the step size is related to the variance of the proposal distribution. See the next section for more information.\n",
    "\n",
    "These are the default numbers that PyMC3 uses, which can be modified. It was reported in a study that the acceptance rate of $23.4%$ results in the highest efficiency for Metropolis Hastings. These are empirical results and therefore should be treated as guidelines. According to the SAS Institute, a high acceptance rate (90% or so) usually is a sign that the new samples are being drawn from points close to the existing point and therefore the sampler is not exploring the space much. On the other hand, a low acceptance rate is probably due to inappropriate proposal distribution causing new samples to be rejected. PyMC3 aims to get an acceptance rate between 20% and 50% for Metropolis Hastings, 65% for Hamiltonian Monte Carlo (HMC) and 85% for No U-Turn Sampler (NUTS).\n",
    "\n",
    "If you have convergence issues as indicated by the visual inspection of the trace, you can try increasing the number of samples used for tuning. It is also worth pointing out that there is more than just step-size adaptation that is happening during this tuning phase.\n",
    "\n",
    "```\n",
    "pm.sample(num_samples, n_tune=num_tuning)\n",
    "```\n",
    "\n",
    "### 1.2.1 Metropolis algorithm\n",
    "\n",
    "In the Metropolis algorithm the standard deviation of the proposal distribution is a tuning parameter that can be set while initializing the algorithm.\n",
    "\n",
    "$$ x_{t+1} ∼ Normal(x_t,stepsize⋅I)$$\n",
    "\n",
    "The larger this value, the larger the space from where new samples can be drawn. If the acceptance rate is too high, increase this standard deviation. Keep in mind that you run the risk of getting invalid draws if it is too large.\n",
    "\n",
    "\n",
    "### 1.2.2 Hamiltonian Monte Carlo (HMC) algorithm\n",
    "\n",
    "Hamiltonian Monte Carlo (HMC) algorithm¶\n",
    "The HMC algorithm is based on the solution of differential equations known as Hamilton’s equations. These differential equations depend on the probability distributions we are trying to learn. We navigate these distributions by moving around them in a trajectory using steps that are defined by a position and momentum at that position. Navigating these trajectories can be a very expensive process and the goal is to minimize this computational effort in this process.\n",
    "\n",
    "To get a sense of the intuition behind HMC, it is based on the notion of conservation of energy. When the sampler trajectory is far away from the probability mass center, it has high potential energy but low kinetic energy and when it is closer to the center of the probability mass, it will have high kinetic energy but low potential energy.\n",
    "\n",
    "The step size, in HMC, corresponds to the covariance of the momentum distribution that is sampled. Smaller step sizes move slowly in the manifold, however larger step sizes can result in integration errors. There is a Metropolis step at the end of the HMC algorithm and the target acceptance rates of 65% in PyMC3 corresponds to this Metropolis step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1.3 Mixing\n",
    "\n",
    "Mixing refers to how well the sampler covers the 'support' of the posterior distribution or rather how well it cover the entire distribution. Poor convergence is often a result of poor mixing. This can happen due to the choice of:\n",
    "\n",
    "1. the choice of an inappropriate proposal distribution for Metropolis.\n",
    "2. If we have too many correlated variables.\n",
    "\n",
    "The underlying cause for this can be:\n",
    "\n",
    "1. Too large step size.\n",
    "2. Not running the sampler long enough.\n",
    "3. Multimodal distributions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 1.4 Rhat\n",
    "\n",
    "**Important: you can see the rhat statistics when sampling in pymc**\n",
    "\n",
    "We can compute a metric called Rhat (also called the potential scale reduction factor) that measures the **ratio of the variance between the chains to the variance within the chains**. It is calculated as the ratio of the standard deviation using the samples from all the chains (all samples appended together from each chain) over the RMS of the within-chain standard deviations of all the chains. \n",
    "\n",
    "* Poorly mixed samples will have greater variance in the accumulated samples (numerator) compared to the variance in the individual chains.\n",
    "\n",
    "It was empirically determined that Rhat values below 1.1 are considered acceptable while those above it are indications of a lack of convergnece in the chains.\n",
    "\n",
    " Germal et al. (2013) introduced a split Rhat that compares the first half with the second half of the samples from each chain to improve upon the regular Rhat.  **Arviz implements a split Rhat** as can be seen from [Arviz Rhat](https://python.arviz.org/en/0.14.0/404.html). There is also in improved rankbased Rhat [Improved Rhat](https://arxiv.org/pdf/1903.08008.pdf).\n",
    "\n",
    " ```\n",
    " az.rhat(trace, method='split)\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2. Centered vs Non-centered \n",
    "\n",
    "[T Wiecki on Reparametrization](https://twiecki.io/blog/2017/02/08/bayesian-hierchical-non-centered/)\n",
    "\n",
    "When there is insufficient data in the hierarchical model, the variables being inferred end up having correlation effects, thereby making it difficult to sample. One obvious solution is to obtain more data, but when this isn't possible we resort to reparameterization by creating a non-centered model from the centered model. \n",
    "\n",
    "## 2.1 Centered Model\n",
    "\n",
    "$$ \\mu ∼ Normal(0,1) $$\n",
    "$$ \\sigma ∼ HalfNormal(1)$$\n",
    "$$ y_i ∼ Normal(\\mu, \\sigma) $$\n",
    "\n",
    "and we try to fit the two parameters for $\\mu$ and $\\sigma$ directly here.\n",
    "\n",
    "\n",
    "## 2.2 Non - Centered Model\n",
    "\n",
    "$$ \\mu ∼ Normal(0,1) $$\n",
    "$$ \\sigma ∼ HalfNormal(1)$$\n",
    "$$ y_{i-unit} ∼ Normal(0, 1)$$\n",
    "$$ y = \\mu + \\sigma*y_{i-unit}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parameterizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Revisiting the multiclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Diagnosing MCMC using PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Arviz Data Represenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
